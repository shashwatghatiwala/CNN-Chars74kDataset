# -*- coding: utf-8 -*-
"""Shashwat_Ghatiwala_AML_HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kGiw1_JJyQDao-RbdaA-JKMNIP7CNqSd
"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

!wget http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/EnglishImg.tgz

!gunzip EnglishImg.tgz

!tar -xvf EnglishImg.tar

import glob
import matplotlib
import pylab
import cv2
import argparse
import os
import string, re
import numpy as np
import random
from matplotlib import pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import imageio as im
from keras import models
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense
from keras.layers import Dropout
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint
from sklearn.metrics import confusion_matrix
import seaborn as sns
from keras.models import model_from_json

CLASSES = string.digits + string.ascii_uppercase + string.ascii_lowercase

def load_filenames(datapath, filters=[]):
    filenames = []
    for path, dirs, files in os.walk(datapath):
        # filtering out unwanted paths
        if sum(map(lambda f : f in path, filters)) == len(filters):
            filenames += list(map(lambda f : path+'/'+f, files))
    return filenames

def split_and_save_dataset(dataset, filename):
    splits = [0.8, 0.2]
    split_names = ['train', 'test']
    perm = np.random.permutation(len(dataset))
    
    for s, split in enumerate(splits):
        startindex = int(sum(splits[:s]) * len(dataset))
        endindex = int(startindex + splits[s] * len(dataset))
        with open(filename+'_'+split_names[s], 'w') as f:
            for i in perm[startindex:endindex]:
                f.write(dataset[i]+'\n')

def get_class_index(filename):
    return int(re.findall(r'.*img(\d+).*', filename)[0]) - 1

def get_class(filename):
    return CLASSES[get_class_index(filename)]

bad_images = []
bad_images = load_filenames('English/Img/BadImag/Bmp', bad_images)
# print(bad_images)

split_and_save_dataset(bad_images, 'bad')

good_images = []
good_images = load_filenames('English/Img/GoodImg/Bmp', good_images)
# print(good_images)
split_and_save_dataset(good_images, 'good')

good_train = open("good_train", "r+")
g = good_train.readlines()
print(len(g))
bad_train = open("bad_train", "r+")
b = bad_train.readlines()
print(len(b))
train = g.extend(b)

train = g
print(len(train))

good_test = open("good_test", "r+")
g = good_test.readlines()
print(len(g))
bad_test = open("bad_test", "r+")
b = bad_test.readlines()
print(len(b))

test = g.extend(b)
test = g
print(len(test))

for i in range(len(train)):
  train[i] = train[i].rstrip()
print(train)

for i in range(len(test)):
  test[i] = test[i].rstrip()

import shutil
train_path = "data/train"
test_path = "data/test"

for line in train:
        splits = line.split('/')
        old_path = splits[:-1]
        categ = old_path[-1:]
        categ = "".join(categ)
        old_path = "/".join(old_path)
        filename = splits[-1:]
        filename = "".join(filename)
        print(line)
        print("Path Name: ", old_path, filename, categ)
        # os.rename(line, train_path + "/" + filename)
        try:
            shutil.copyfile(line, train_path + "/" + categ + "/" + filename)
        except FileNotFoundError:
            os.makedirs(train_path + "/" + categ)
            shutil.copyfile(line, train_path + "/" + categ + "/" + filename)
            
            
for line in test:
        splits = line.split('/')
        old_path = splits[:-1]
        categ = old_path[-1:]
        categ = "".join(categ)
        old_path = "/".join(old_path)
        filename = splits[-1:]
        filename = "".join(filename)
        print(line)
        print("Path Name: ", old_path, filename, categ)
        # os.rename(line, train_path + "/" + filename)
        try:
            shutil.copyfile(line, test_path + "/" + categ + "/" + filename)
        except FileNotFoundError:
            os.makedirs(test_path + "/" + categ)
            shutil.copyfile(line, test_path + "/" + categ + "/" + filename)

train_datagen = ImageDataGenerator(
        rescale=1./255)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
        'data/train',
        target_size=(32, 32),
        batch_size=32,
        color_mode = 'grayscale',
        class_mode='categorical', seed = 42)

test_generator = test_datagen.flow_from_directory(
    'data/test',
    target_size=(32, 32),
    color_mode="grayscale",
    batch_size=32,
    class_mode='categorical',
    shuffle=False,
    seed=42
)

print('Creating the model')
model = Sequential()

model.add(Conv2D(32,3,strides = 1, input_shape =(32, 32, 1), activation='relu', use_bias=True))
model.add(MaxPooling2D(pool_size=(2,2), strides = (2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, 5, strides=1, activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides = (2,2)))
model.add(Dropout(0.25))

model.add(Flatten())

model.add(Dense(256, activation = 'relu'))
model.add(Dropout(0.5))

model.add(Dense(62, activation = 'softmax'))

model.summary()

from keras.optimizers import SGD
sgd = SGD(lr=0.03)
model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])

checkpointer = ModelCheckpoint(filepath="best_weights.hdf5", 
                               monitor = 'val_acc')

Training_Fit = model.fit_generator(
        train_generator,
        steps_per_epoch=6940/32,
        epochs=150, callbacks = [checkpointer])

model.evaluate_generator(generator = test_generator, steps = 2311/32)

model.load_weights('best_weights.hdf5')
with open("cnn.json", "w") as json_file:
    json_file.write(model_json)
model.save_weights("cnn.h5")
print("Saved model to disk")

json_file = open('cnn.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
loaded_model.load_weights("cnn.h5")

loaded_model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])

test_generator.reset()
y_prediction = loaded_model.predict_generator(test_generator, steps = 2311/32)
y_classes = np.argmax(y_prediction, axis =1)
y_true = test_generator.classes

# print(len(y_true))
# print(len(y_classes))

loss, acc = loaded_model.evaluate_generator(test_generator, steps=test_generator.n)
print("Summary of constructed CNN performance on Test Data:\n Loss = {}, Accuracy = {} ".format(loss, acc))

def ConfusionMatrix(y_true, y_classes):
    confusion = confusion_matrix(y_true,y_classes)
    print("confusion: ", confusion)
    plt.figure(figsize=(40,20))
    plt.title('Confusion Matrix for Test Data', fontsize=20)
    plt.xlabel('Predicted Values', fontsize=16)
    plt.ylabel('True Values', fontsize=16)
    sns.heatmap(confusion, annot=True, fmt="d", center= 0)
    plt.show()

def FeatureMaps():
    img_path = "data/test/Sample004/img004-00003.png"
    img = image.load_img(img_path, color_mode='grayscale', target_size=(32, 32))
    img_tensor = image.img_to_array(img)
    img_tensor = np.expand_dims(img_tensor, axis=0)
    img_tensor /= 255.
    print(img_tensor.shape)

    layer_outputs = [layer.output for layer in loaded_model.layers[:12]]
    activation_model = models.Model(inputs=loaded_model.input, outputs=layer_outputs)
    activations = activation_model.predict(img_tensor)

    layer_names = []
    # print(type(model.layers))
    for layer in loaded_model.layers[:-6]:
        #    print(layer)
        layer_names.append(layer.name)  # Names of the layers, so you can have them as part of your plot

    images_per_row = 16

    for layer_name, layer_activation in zip(layer_names, activations):  # Displays the feature maps
        n_features = layer_activation.shape[-1]  # Number of features in the feature map
        size = layer_activation.shape[1]  # The feature map has shape (1, size, size, n_features).
        n_cols = n_features // images_per_row  # Tiles the activation channels in this matrix
        display_grid = np.zeros((size * n_cols, images_per_row * size))
        for col in range(n_cols):  # Tiles each filter into a big horizontal grid
            for row in range(images_per_row):
                channel_image = layer_activation[0,
                                :, :,
                                col * images_per_row + row]
                channel_image -= channel_image.mean()  # Post-processes the feature to make it visually palatable
                channel_image /= channel_image.std()
                channel_image *= 64
                channel_image += 128
                channel_image = np.clip(channel_image, 0, 255).astype('uint8')
                display_grid[col * size: (col + 1) * size,  # Displays the grid
                row * size: (row + 1) * size] = channel_image
        scale = 1. / size
        plt.figure(figsize=(scale * display_grid.shape[1],
                            scale * display_grid.shape[0]))
        plt.title(layer_name)
        plt.grid(False)
        plt.imshow(display_grid, aspect='auto', cmap='viridis')

FeatureMaps()

plt.show()

ConfusionMatrix(y_true, y_classes)